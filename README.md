# 如何用深度强化学习PPO算法做量化交易

## 💡 一切的开始

22年大二那段时间，上海因为疫情锁在寝室无聊，我本身做过几年的交易，而且学的专业也是AI，于是产生了用强化学习做量化交易的念头。

可是我翻遍了Github，还有一些人家写的论文，代码要么不够专业（与现实交易规则环境不符），要么不公开代码，基本上找不到多少能够复现的强化学习量化交易项目。

在这种情况下，我完成了符合现实交易环境的强化学习量化交易项目。

我现在大三，看到我电脑上一堆学习笔记式的代码项目，又想起我当初找强化学习量化开源项目的时候的无助感。

于是我决定将当初的强化学习量化交易系统一部分开源，作为我第一个开源的项目，以保证后来者能够更轻松的实现自己的想法，让大家更加轻松的进入量化这个领域（中国股市量化的资金量已经占到40%-50%了，还不冲吗？）


## 📖 什么是强化学习量化？

强化学习是一种人工智能领域的学习方法，它是通过让智能体与环境互动来学习决策的最佳方式。强化学习通过试错来学习，即在不断尝试实现目标的过程中，机器对行为的优劣进行奖励或惩罚，来优化决策方法，以获得更高的回报。

量化是指通过收集和分析大量数据来制定投资策略的一种方法。量化投资通常使用数学、统计学和计算机科学等技术来分析市场数据，以寻找股票和其他金融产品的价格模式和趋势。量化投资的目标是获得更高的收益和降低风险。

强化学习与量化的结合可以帮助投资者更好地理解市场行为和规律，以制定更准确的投资策略，提高投资回报和降低风险。通过强化学习的奖励机制和量化投资的数据分析，机器能够自动地学习和优化投资决策，提高投资效率和准确性。

## 🤖 期货交易环境

本项目选择甲醛连续主力期货作为交易环境，交易周期为1分钟K线日内交易。起始资金量为10000元，最大持仓手数1手，可做多空两个方向。一手甲醛所需要的资金为 `当前价格*10*0.12` 也就是8.33倍杠杆（每个人的杠杆不一样，需情况而定）。

甲醛交易手续费：开仓一手2+0.01元，平今仓一手6+0.01元，平昨仓一手2+0.01元。

### 观测值 Observation

Observation是强化学习模型所观测的值，模型依据这些数据来做出决策。本项目观测多个变化值，包括基础的K线数据、维度时间数据、账户情况数据等。

| 参数名称                 | 参数描述    | 说明    |
|----------------------|---------|-------|
| balance              | 账户现金    | 单位：元  |
| shares_held          | 持仓数     | 单位：手  |
| cost_basis           | 购买成本    | 单位：元  |
| total_shares_sold    | 总共卖出手数  | 单位：元  |
| open                 | 开盘价格    | 单位：元  |
| high                 | 最高价     | 单位：元  |
| low                  | 最低价     | 单位：元  |
| close                | 收盘价     | 单位：元  |
| volume               | 成交手数    | 单位：手  |
| money                | 成交量     | 单位：元  |
| open_interest        | 未平仓手数   | 单位：手  |
| month                | 月份      | 单位：月  |
| day                  | 天数      | 单位：天  |
| weekday              | 星期几     | 单位：周  |
| hour                 | 小时      | 单位：小时 |
| minute               | 分钟      | 单位：分钟 |
| return_15minute      | close特征 | 单位：无  |
| return_30minute      | close特征     | 单位：无  |
| return_60minute      | close特征   | 单位：无  |
| return_120minute     | close特征      | 单位：无  |
| volatility_15minute  | close特征 | 单位：无  |
| volatility_30minute  | close特征     | 单位：无  |
| volatility_60minute  | close特征   | 单位：无  |
| volatility_120minute | close特征      | 单位：无  |
| MA_gap_15minute      | close特征 | 单位：无  |
| MA_gap_30minute      | close特征     | 单位：无  |
| MA_gap_60minute      | close特征   | 单位：无  |
| MA_gap_120minute     | close特征      | 单位：无  |

### 动作 Action

有买入，卖出，持仓三种动作，以保证多头，空头，空仓三种状态。

- `action[-1]` 表示卖出，如果持空仓，为买入空头。如果持多头，为买入平仓；
- `action[0]` 表示持仓；
- `action[1]` 表示买入，如果持空头，为卖出平仓。如果持空仓，为买入多头；

| 动作类型 `action[0]` | 说明              |
|---|-----------------|
| 1 | 买入 `action[1]`  |
| 2 | 卖出 `action[-1]` |
| 3 | 保持 `action[0]`  |

### 奖励 Reward

奖励函数对强化学习的目标至关重要。在这里我采用一个简单的奖励函数作为示例，当前时刻与上一时刻的利润差，即`当前总市值 - 上一时刻总市值 = 利润`。

```python
# profits
reward = self.net_worth - self.worth
```

### 模型 model

本项目采用informer模型为主干模型[论文](https://arxiv.org/pdf/2012.07436) ，DataEmbedding来融合时间信息与K线数据信息，再利用Multihead_Conv模型来分别提取每一个时间维度上的信息，最后再送入informer的encoder部分提取特征

强化学习训练方法为PPO模型（近端策略优化算法），它是目前非常流行的策略梯度算法之一。PPO是一种改进的策略梯度方法，主要是通过限制策略更新的幅度和大小，来在不破坏稳定性的前提下提高训练速度和性能。chatgpt用了PPO的训练方法，区别便是它将奖励函数换成了一个神经网络，而不是传统的一个自定义函数，所以未来大家的方向可以集中这一个方向。




## 🕵️‍♀️ 模拟实验

### 环境安装

```sh
# 虚拟环境
virtualenv -p python3.9 venv
source ./venv/bin/activate
# 安装库依赖
pip install -r requirements.txt
```

### 快速实现
训练
```sh
python train.py
```
测试
```sh
python test.py
```

### 数据可用性
数据分享到百度网盘：[链接](https://pan.baidu.com/s/1O1xNZWPHmfKgWSqeaguG_g?pwd=g2ik)

也可以通过python包来实现，具体请查看[官网](https://efinance.readthedocs.io/en/latest/index.html)
```sh
pip install efinance
```


## 参考资料

https://github.com/notadamking/Stock-Trading-Environment

https://efinance.readthedocs.io/en/latest/index.html

https://arxiv.org/pdf/2012.07436
